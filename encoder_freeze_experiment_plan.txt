エンコーダ凍結設定の拡張計画（単一フラグ必須版）
====================================

目的
----
論文実験で「エンコーダを凍結」「エンコーダも微調整」の両条件を比較できるようにしつつ、設定フラグを 1 つに統一し、未設定ならエラーにすることで実験条件を明示的に管理する。

1. 設定項目の追加
   - `src/config/config.json` にブール値 `freeze_encoder` を追加し、必ず `true` または `false` を指定する運用に切り替える。
   - `trainer.trainer` 冒頭で `if "freeze_encoder" not in config: raise ValueError("freeze_encoder must be specified as true/false in config")` のように存在チェックを入れる。Hydra や CLI からも同名パラメータを必ず渡す。

2. コントラスト学習＋線形評価の分岐
   - `eval_model` のシグネチャに `freeze_encoder: bool` を追加し、呼び出し側から `config["freeze_encoder"]` を渡す。
   - `freeze_encoder` が `true` の場合は従来どおり `create_dataloader_hidden_space` を使って特徴を書き出し、線形層のみを学習する。
   - `freeze_encoder` が `false` の場合は以下のようにエンド・ツー・エンドで学習する：
     1. `src/trainer/basic_utils.py` に `traine_linear_classifier_end_to_end` を追加し、線形層とエンコーダを同時に更新するループを実装する。
     2. 元の DataLoader バッチを用いて `model` を `train()`、`LinearEvaluation` で BCE-with-logits 損失を計算し、`optimizer` で両方のパラメータを更新する。
     3. Optimizer パラメータ群は `model.parameters_training(lr_backbone=config["lr"], lr_projection=config["lr_adding"], wd=config["wd"])` と線形層 `parameters_training(lr=lr, wd=wd)` を結合し、既存の `lr`／`lr_adding`／`wd` のみで制御する。
   - 学習後の評価 (`torch.no_grad()`) は従来どおり `create_dataloader_hidden_space` で行い、計測ロジックは流用する。

3. BCE／非コントラスト学習側
   - `train_BCE` で `freeze_flag = config["freeze_encoder"]` として受け取り、`true` の場合のみバックボーンを凍結する（`requires_grad_(False)` と `eval()` を実行）。
   - Optimizer には学習可能なパラメータのみを渡す。`freeze_flag` が `true` のときは `bce_model.backbone` をパラメータリストから除外する実装に修正する。
   - wandb ログに `encoder_frozen` を追加し、実験ごとに条件が判別できるようにする。

4. 周辺作業とテスト
   - すべての設定ファイル・スクリプト・ノートブックで `freeze_encoder` を明示的に指定するよう更新する。
   - 動作確認：
     1. `freeze_encoder=true` で線形評価時も BCE 時もエンコーダが凍結され、勾配がゼロであることを確認。
     2. `freeze_encoder=false` で両パイプラインがエンコーダを更新し、バックボーンに勾配が流れることを確認。
   - 実験 README や論文付録に、このフラグを必須パラメータとして明記し、どの条件で実験したかを記録する。

この構成により、単一の `freeze_encoder` フラグで全パイプラインを統一制御でき、設定漏れによる曖昧な実験条件を防げる。
